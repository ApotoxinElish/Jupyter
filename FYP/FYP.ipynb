{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfOKN1k19_JM",
        "outputId": "aa34ed16-2c67-4ab5-8bc0-e1fad98ebc7c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertConfig, BertModel, BertTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_hUbOIkY-fRu"
      },
      "outputs": [],
      "source": [
        "def read_json(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
        "        for line in fp:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ec4yWClZ-fW6"
      },
      "outputs": [],
      "source": [
        "class BaselineData(Dataset):\n",
        "    def __init__(self, data, tokenizer, config):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_size = config.pad_size\n",
        "        self.label2id = config.label2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = \"[SEP]\".join(\n",
        "            [self.data[idx][\"hashtag\"]] + self.data[idx][\"comments\"]\n",
        "        )\n",
        "\n",
        "        input_ids, attention_mask = self.__convert_to_id__(sentence)\n",
        "\n",
        "        if self.data[idx].get(\"attitudes\"):\n",
        "            label = self.__convert_label__(self.data[idx][\"attitudes\"])\n",
        "            return (\n",
        "                torch.tensor(input_ids),\n",
        "                torch.tensor(attention_mask),\n",
        "                torch.tensor(label),\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                torch.tensor(input_ids),\n",
        "                torch.tensor(attention_mask),\n",
        "            )\n",
        "\n",
        "    def __convert_to_id__(self, sentence):\n",
        "        ids = self.tokenizer.encode_plus(sentence)\n",
        "        input_ids = self.__padding__(ids[\"input_ids\"])\n",
        "        attention_mask = self.__padding__(ids[\"attention_mask\"])\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    def __convert_label__(self, label):\n",
        "        onehot_label = [0] * 24\n",
        "        for i in label:\n",
        "            onehot_label[self.label2id[i]] = 1\n",
        "        return onehot_label\n",
        "\n",
        "    def __padding__(self, sentence):\n",
        "        sentence = sentence[: self.pad_size]  # 长就截断\n",
        "        sentence = sentence + [0] * (self.pad_size - len(sentence))  # 短就补充\n",
        "        return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XS5_wE0l-fa6"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.bert = BertModel.from_pretrained(config.PTM)\n",
        "        self.bert_config = BertConfig.from_pretrained(config.PTM)\n",
        "        self.fc = nn.Linear(self.bert_config.hidden_size, self.bert_config.hidden_size)\n",
        "        self.fc1 = nn.Linear(self.bert_config.hidden_size, config.label_num)\n",
        "        self.act = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bert(input_ids=x[0], attention_mask=x[1]).pooler_output\n",
        "        x = self.sigmoid(self.fc1(self.act(self.fc(x))))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Lg9YwUQi-fds"
      },
      "outputs": [],
      "source": [
        "def train(config, dataset, model, optimizer, valid_dataset):\n",
        "    max_scores = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        with tqdm(total=len(dataset)) as pbar:\n",
        "            for idx, data in enumerate(dataset):\n",
        "                x = [data[0].long().to(config.device), data[1].long().to(config.device)]\n",
        "                y = data[2].float().to(config.device)\n",
        "                y_hat = model(x)\n",
        "                loss = F.binary_cross_entropy(y_hat, y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                pbar.set_postfix({\"loss\": \"{:.4f}\".format(loss)})\n",
        "                pbar.update(1)\n",
        "        scores = valid(config, valid_dataset, model)\n",
        "        if scores >= max_scores:\n",
        "            max_scores = scores\n",
        "            saved_model = model\n",
        "    return saved_model\n",
        "\n",
        "\n",
        "def valid(config, dataset, model):\n",
        "    true = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataset):\n",
        "            x = [data[0].long().to(config.device), data[1].long().to(config.device)]\n",
        "            y = data[2].float().to(config.device).view(-1, 24).tolist()\n",
        "            y_hat = model(x).view(-1, 24).tolist()\n",
        "            true.extend(y)\n",
        "            pred.extend(y_hat)\n",
        "\n",
        "    pred = [[1 if i >= 0.5 else 0 for i in j] for j in pred]\n",
        "\n",
        "    micro_f1 = f1_score(pred, true, average=\"micro\")\n",
        "    macro_f1 = f1_score(pred, true, average=\"macro\")\n",
        "    print(\"micro_f1: {:.4f}\".format(micro_f1))\n",
        "    print(\"micro_f1: {:.4f}\".format(macro_f1))\n",
        "    return micro_f1 * 0.4 + macro_f1 * 0.6\n",
        "\n",
        "\n",
        "import heapq\n",
        "\n",
        "\n",
        "def generate_test_result(config, dataset, model):\n",
        "    with torch.no_grad():\n",
        "        predict = []\n",
        "        for idx, data in enumerate(dataset):\n",
        "            x = [data[0].long().to(config.device), data[1].long().to(config.device)]\n",
        "            predict.extend(model(x).view(-1, 24).tolist())\n",
        "    with open(\"submit.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(predict)):\n",
        "            line = []\n",
        "\n",
        "            max_number = heapq.nlargest(3, predict[i])\n",
        "            # max_index = []\n",
        "            for t in max_number:\n",
        "                index = predict[i].index(t)\n",
        "                # max_index.append(index)\n",
        "                line.append(config.id2label[index])\n",
        "\n",
        "            # for j in range(len(predict[i])):\n",
        "            #     if predict[i][j] >= 0.5:\n",
        "            #         line.append(config.id2label[j])\n",
        "\n",
        "            f.write(\" \".join([str(i)] + line))\n",
        "            f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7891892194747925, 0.618352472782135, 0.47227057814598083]\n",
            "[13, 14, 10]\n"
          ]
        }
      ],
      "source": [
        "import heapq\n",
        "\n",
        "m = [\n",
        "    0.016138209030032158,\n",
        "    0.1027437224984169,\n",
        "    0.007721974980086088,\n",
        "    0.015482794493436813,\n",
        "    0.03860097751021385,\n",
        "    0.01681477390229702,\n",
        "    0.0018505491316318512,\n",
        "    0.2727064788341522,\n",
        "    0.10750023275613785,\n",
        "    0.10568385571241379,\n",
        "    0.47227057814598083,\n",
        "    0.059659115970134735,\n",
        "    0.02180691622197628,\n",
        "    0.7891892194747925,\n",
        "    0.618352472782135,\n",
        "    0.23794208467006683,\n",
        "    0.011238433420658112,\n",
        "    0.11568883806467056,\n",
        "    0.07733272016048431,\n",
        "    0.009712048806250095,\n",
        "    0.011674805544316769,\n",
        "    0.12361800670623779,\n",
        "    0.00770289683714509,\n",
        "    0.055381402373313904,\n",
        "]\n",
        "max_number = heapq.nlargest(3, m)\n",
        "max_index = []\n",
        "for t in max_number:\n",
        "    index = m.index(t)\n",
        "    max_index.append(index)\n",
        "print(max_number)\n",
        "print(max_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ItHXTZ7eCPs2"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.pad_size = 500\n",
        "        self.batch_size = 24\n",
        "        self.epochs = 15\n",
        "        self.PTM = \"bert-base-chinese\"\n",
        "        self.label_num = 24\n",
        "        self.device = \"cuda:0\"\n",
        "        self.lr = 5e-5\n",
        "\n",
        "        label_dic = [\n",
        "            \"[微笑]\",\n",
        "            \"[嘻嘻]\",\n",
        "            \"[笑cry]\",\n",
        "            \"[怒]\",\n",
        "            \"[泪]\",\n",
        "            \"[允悲]\",\n",
        "            \"[憧憬]\",\n",
        "            \"[doge]\",\n",
        "            \"[并不简单]\",\n",
        "            \"[思考]\",\n",
        "            \"[费解]\",\n",
        "            \"[吃惊]\",\n",
        "            \"[拜拜]\",\n",
        "            \"[吃瓜]\",\n",
        "            \"[赞]\",\n",
        "            \"[心]\",\n",
        "            \"[伤心]\",\n",
        "            \"[蜡烛]\",\n",
        "            \"[给力]\",\n",
        "            \"[威武]\",\n",
        "            \"[跪了]\",\n",
        "            \"[中国赞]\",\n",
        "            \"[给你小心心]\",\n",
        "            \"[酸]\",\n",
        "        ]\n",
        "\n",
        "        self.id2label = {k: v for k, v in enumerate(label_dic)}  # 用于标签的部分\n",
        "        self.label2id = {v: k for k, v in enumerate(label_dic)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NdzMgbaxCTaF"
      },
      "outputs": [],
      "source": [
        "config = Config()\n",
        "train_data = read_json(\"train.json\")\n",
        "valid_data = read_json(\"valid.json\")\n",
        "test_data = read_json(\"test.json\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(config.PTM)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    BaselineData(train_data, tokenizer, config), batch_size=config.batch_size\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    BaselineData(valid_data, tokenizer, config), batch_size=config.batch_size\n",
        ")\n",
        "test_dataloader = DataLoader(BaselineData(test_data, tokenizer, config), batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': '任嘉伦谭松韵互叫本名',\n",
              " 'attitudes': ['[笑cry]', '[允悲]', '[心]'],\n",
              " 'comments': ['晶晶啊晶晶啊',\n",
              "  '哈哈哈哈哈哈两个人好可爱',\n",
              "  '突然觉得自己看锦衣之下之前就是个瞎子',\n",
              "  '喊了四个晶晶啊，因为锦衣之下，因为陆大人今夏，嘉伦松韵成了家人一样的亲切关系，祝福友谊之树长青。@任嘉伦Allen @谭松韵seven @锦衣之下电视剧',\n",
              "  '国超“晶晶啊，晶晶啊，晶晶啊”，晶晶“哎”，国超“你可以搜一下任嘉伦”，晶晶“我嗖啦”。']}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[8]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': '武汉一餐厅疫情期送8000多份爱心餐',\n",
              " 'attitudes': ['[心]', '[泪]', '[赞]'],\n",
              " 'comments': ['餐厅名字，三串肆季  王家墩 送我上去',\n",
              "  '活该你店火🔥你知道吧',\n",
              "  '很棒的母亲',\n",
              "  '总有人竭力温暖这个世界！致敬！可敬的母亲',\n",
              "  '可敬的妈妈，您的孩子一定以你为榜样的，母亲节快乐❤️']}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[37]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': '近视眼的日常',\n",
              " 'attitudes': ['[允悲]', '[泪]', '[doge]'],\n",
              " 'comments': ['脑洞翻唱 【wherever you are→http://t.cn/AiO9oJhZ】 【肥宅的惰性→http://t.cn/AiOSBn6m】【星巴克的约定→http://t.cn/AiWqi9aB】 【吃货肥宅们的日常→http://t.cn/AiWqi9am】【熊孩纸→ http://t.cn/AiF2FclT】【理发店→ http://t.cn/AiF2FclN】【做饭难吃→ http://t.cn/A6hSy9ZY】',\n",
              "  '你說什麼等等我沒戴眼鏡聽不清',\n",
              "  '以前做视力检查，问过医生，指视力表的棍子在哪（眯眼',\n",
              "  '曾经没戴眼镜，以为是叶子，结果徒手抓了只壁虎',\n",
              "  '  “夜晚的灯光好美啊 看起来像彩灯秀”太真实了吧！！！散光眼表示摘下眼镜 可以被路灯晃晕']}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[44]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': '近视眼的日常',\n",
              " 'attitudes': ['[允悲]', '[泪]', '[doge]'],\n",
              " 'comments': ['脑洞翻唱 【wherever you are→】 【肥宅的惰性→】【星巴克的约定→】 【吃货肥宅们的日常→】【熊孩纸→ 】【理发店→ 】【做饭难吃→ 】',\n",
              "  '你說什麼等等我沒戴眼鏡聽不清',\n",
              "  '以前做视力检查，问过医生，指视力表的棍子在哪（眯眼',\n",
              "  '曾经没戴眼镜，以为是叶子，结果徒手抓了只壁虎',\n",
              "  '“夜晚的灯光好美啊 看起来像彩灯秀”太真实了吧！！！散光眼表示摘下眼镜 可以被路灯晃晕']}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# @ with : and @ without :\n",
        "def clean_at(text):\n",
        "    at_pattern = re.compile(\"@\\S*:\", re.S)\n",
        "    text = re.sub(at_pattern, \"\", text)\n",
        "    at_pattern = re.compile(\"@\\S*\", re.S)\n",
        "    text = re.sub(at_pattern, \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# Clear url in comments\n",
        "def clean_url(text):\n",
        "    sentences = text.split(\" \")\n",
        "    # 处理http://类链接\n",
        "    url_pattern = re.compile(r\"(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b\", re.S)\n",
        "    # 处理无http://类链接\n",
        "    domain_pattern = re.compile(r\"(\\b)*(.*?)\\.(com|cn)\")\n",
        "    if len(sentences) > 0:\n",
        "        result = []\n",
        "        for item in sentences:\n",
        "            text = re.sub(url_pattern, \"\", item)\n",
        "            text = re.sub(domain_pattern, \"\", text)\n",
        "            result.append(text)\n",
        "        return \" \".join(result)\n",
        "    else:\n",
        "        return re.sub(url_pattern, \"\", sentences)\n",
        "\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    for j in range(len(train_data[i][\"comments\"])):\n",
        "        text = train_data[i][\"comments\"][j]\n",
        "        text_clean_at = clean_at(text)\n",
        "        text_clean_url = clean_url(text_clean_at)\n",
        "        train_data[i][\"comments\"][j] = text_clean_url\n",
        "\n",
        "train_data[44]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YldSf9yy-7X3",
        "outputId": "0bbe51f8-c3b9-49e2-8e89-e4769832a01f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = Model(config).to(config.device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), config.lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWJVjUIf-7aW",
        "outputId": "994d8a68-f072-4c13-cf54-99fa78ad6423"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/300 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model \u001b[39m=\u001b[39m train(config, train_dataloader, model, optimizer, valid_dataloader)\n\u001b[0;32m      2\u001b[0m generate_test_result(config, test_dataloader, best_model)\n",
            "Cell \u001b[1;32mIn[23], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config, dataset, model, optimizer, valid_dataset)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset)) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[1;32m----> 6\u001b[0m         x \u001b[39m=\u001b[39m [data[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mto(config\u001b[39m.\u001b[39;49mdevice), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)]\n\u001b[0;32m      7\u001b[0m         y \u001b[39m=\u001b[39m data[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m      8\u001b[0m         y_hat \u001b[39m=\u001b[39m model(x)\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "best_model = train(config, train_dataloader, model, optimizer, valid_dataloader)\n",
        "generate_test_result(config, test_dataloader, best_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "label_dic = [\n",
        "    \"[微笑]\",\n",
        "    \"[嘻嘻]\",\n",
        "    \"[笑cry]\",\n",
        "    \"[怒]\",\n",
        "    \"[泪]\",\n",
        "    \"[允悲]\",\n",
        "    \"[憧憬]\",\n",
        "    \"[doge]\",\n",
        "    \"[并不简单]\",\n",
        "    \"[思考]\",\n",
        "    \"[费解]\",\n",
        "    \"[吃惊]\",\n",
        "    \"[拜拜]\",\n",
        "    \"[吃瓜]\",\n",
        "    \"[赞]\",\n",
        "    \"[心]\",\n",
        "    \"[伤心]\",\n",
        "    \"[蜡烛]\",\n",
        "    \"[给力]\",\n",
        "    \"[威武]\",\n",
        "    \"[跪了]\",\n",
        "    \"[中国赞]\",\n",
        "    \"[给你小心心]\",\n",
        "    \"[酸]\",\n",
        "]\n",
        "id2label = {k: v for k, v in enumerate(label_dic)}  # 用于标签的部分\n",
        "label2id = {v: k for k, v in enumerate(label_dic)}\n",
        "\n",
        "\n",
        "def convert_label(fn_result):\n",
        "    convert_label = []\n",
        "\n",
        "    for line in open(fn_result, \"r\", encoding=\"utf-8\"):\n",
        "        labellist = line.strip().split(\" \")[1:]\n",
        "\n",
        "        onehot_label = [0] * 24\n",
        "        for label in labellist:\n",
        "            onehot_label[label2id[label]] = 1\n",
        "        convert_label.append(onehot_label)\n",
        "    return convert_label\n",
        "\n",
        "\n",
        "def score_calculation(pred, true):\n",
        "    macro_f1 = f1_score(pred, true, average=\"macro\")\n",
        "    return macro_f1\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Generate classification_report from given pred and gold tsv files.\n",
        "    \"\"\"\n",
        "\n",
        "    # Usage:\n",
        "    # python evaluate PREDICTION_FILE GOLD_ANSWER_FILE\n",
        "    # Example:\n",
        "    # python evaluate pred.tsv gold.tsv\n",
        "    print(len(sys.argv))\n",
        "    print(\"The list of command line arguments:\\n\", sys.argv)\n",
        "\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Please indicate the prediction and gold tsvs.\")\n",
        "        quit()\n",
        "    else:\n",
        "        pred_fn = sys.argv[1]\n",
        "        gold_fn = sys.argv[2]\n",
        "\n",
        "    print(\"Loading the datasets ...\")\n",
        "    pred_lbl = convert_label(pred_fn)\n",
        "    gold_lbl = convert_label(gold_fn)\n",
        "\n",
        "    print(\"Evaluating ...\")\n",
        "    try:\n",
        "        macro_f1 = score_calculation(pred_lbl, gold_lbl)\n",
        "\n",
        "        print(\"macro_f1: {:.4f}\".format(macro_f1))\n",
        "    except Exception as ex:\n",
        "        print(\"error:\", ex)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
