{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = read_json(\"data_emotion/train.json\")\n",
    "valid_data = read_json(\"data_emotion/valid.json\")\n",
    "test_data = read_json(\"data_emotion/test_1.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.pad_size = 500\n",
    "        self.batch_size = 24\n",
    "        self.epochs = 10\n",
    "        self.PTM = \"bert-base-chinese\"\n",
    "        self.label_num = 24\n",
    "        self.device = \"cuda:0\"\n",
    "        self.lr = 5e-5\n",
    "\n",
    "        label_dic = [\n",
    "            \"[微笑]\",\n",
    "            \"[嘻嘻]\",\n",
    "            \"[笑cry]\",\n",
    "            \"[怒]\",\n",
    "            \"[泪]\",\n",
    "            \"[允悲]\",\n",
    "            \"[憧憬]\",\n",
    "            \"[doge]\",\n",
    "            \"[并不简单]\",\n",
    "            \"[思考]\",\n",
    "            \"[费解]\",\n",
    "            \"[吃惊]\",\n",
    "            \"[拜拜]\",\n",
    "            \"[吃瓜]\",\n",
    "            \"[赞]\",\n",
    "            \"[心]\",\n",
    "            \"[伤心]\",\n",
    "            \"[蜡烛]\",\n",
    "            \"[给力]\",\n",
    "            \"[威武]\",\n",
    "            \"[跪了]\",\n",
    "            \"[中国赞]\",\n",
    "            \"[给你小心心]\",\n",
    "            \"[酸]\",\n",
    "        ]\n",
    "\n",
    "        self.id2label = {k: v for k, v in enumerate(label_dic)}  # 用于标签的部分\n",
    "        self.label2id = {v: k for k, v in enumerate(label_dic)}\n",
    "\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.PTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineData(Dataset):\n",
    "    def __init__(self, data, tokenizer, config):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_size = config.pad_size\n",
    "        self.label2id = config.label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = \"[SEP]\".join(\n",
    "            [self.data[idx][\"hashtag\"]] + self.data[idx][\"comments\"]\n",
    "        )\n",
    "\n",
    "        input_ids, attention_mask = self.__convert_to_id__(sentence)\n",
    "\n",
    "        if self.data[idx].get(\"attitudes\"):\n",
    "            label = self.__convert_label__(self.data[idx][\"attitudes\"])\n",
    "            return (\n",
    "                torch.tensor(input_ids),\n",
    "                torch.tensor(attention_mask),\n",
    "                torch.tensor(label),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.tensor(input_ids),\n",
    "                torch.tensor(attention_mask),\n",
    "            )\n",
    "\n",
    "    def __convert_to_id__(self, sentence):\n",
    "        ids = self.tokenizer.encode_plus(sentence)\n",
    "        input_ids = self.__padding__(ids[\"input_ids\"])\n",
    "        attention_mask = self.__padding__(ids[\"attention_mask\"])\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def __convert_label__(self, label):\n",
    "        onehot_label = [0] * 24\n",
    "        for i in label:\n",
    "            onehot_label[self.label2id[i]] = 1\n",
    "        return onehot_label\n",
    "\n",
    "    def __padding__(self, sentence):\n",
    "        sentence = sentence[: self.pad_size]  # 长就截断\n",
    "        sentence = sentence + [0] * (self.pad_size - len(sentence))  # 短就补充\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baselinedata = BaselineData(train_data, tokenizer, config)\n",
    "# valid_baselinedata = BaselineData(valid_data, tokenizer, config)\n",
    "# test_baselinedata = BaselineData(test_data, tokenizer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"[SEP]\".join([train_data[0][\"hashtag\"]] + train_data[0][\"comments\"])\n",
    "len(sentence)\n",
    "ids = tokenizer.encode_plus(sentence)\n",
    "ids\n",
    "# len(ids[\"input_ids\"])\n",
    "# ids[\"input_ids\"][:500]\n",
    "# ids[\"input_ids\"] + [0] * (500 - len(ids[\"input_ids\"]))\n",
    "# len(ids[\"attention_mask\"])\n",
    "# ids[\"attention_mask\"][:500]\n",
    "# ids[\"attention_mask\"] + [0] * (500 - len(ids[\"attention_mask\"]))\n",
    "# train_data[0].get(\"attitudes\")\n",
    "\n",
    "# torch.tensor(ids[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 5908, 5812, 7313,  166, 8118, 8220, 8178,  102, 3342, 2780,  800,\n",
       "         1922, 1377, 4263,  749, 1449, 1449, 1449, 8024, 3633, 1762, 4692, 8013,\n",
       "         2769, 6158, 5846, 7835,  749, 8013,  102,  872, 3297, 2358, 8013, 8013,\n",
       "         8013, 5710, 2552, 5288, 4125, 4306, 8013, 8013, 8013, 2218, 3221,  872,\n",
       "         8013, 8013, 8013, 8013,  102, 1506, 1506, 1506, 1506, 1506, 8024, 3145,\n",
       "         1377, 4263, 4638, 3342, 2140, 2140,  102, 3342, 2140, 2140, 1922, 1377,\n",
       "         4263, 2685,  102, 1745, 4275, 6397, 6389,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_baselinedata[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    BaselineData(train_data, tokenizer, config), batch_size=config.batch_size\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    BaselineData(valid_data, tokenizer, config), batch_size=config.batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(BaselineData(test_data, tokenizer, config), batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "text = \"回复@魏小河才不是乖乖的小盆宇:完全控制以前都不能放松\"\n",
    "at_pattern = re.compile(\"@\\S*:\", re.S)\n",
    "text = re.sub(at_pattern, \"\", text)\n",
    "at_pattern = re.compile(\"@\\S*:\", re.S)\n",
    "text = re.sub(at_pattern, \"\", text)\n",
    "print(text.strip())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "\n",
    "def read_json(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = read_json(\"train.json\")\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"[SEP]\".join([train_data[1382][\"hashtag\"]] + train_data[1382][\"comments\"])\n",
    "sentence\n",
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.pad_size = 500\n",
    "        self.batch_size = 24\n",
    "        self.epochs = 1\n",
    "        self.PTM = \"Yuang/unilm-base-chinese-news-sum\"\n",
    "        self.label_num = 24\n",
    "        self.device = \"cuda:0\"\n",
    "        self.lr = 5e-5\n",
    "\n",
    "        label_dic = [\n",
    "            \"[微笑]\",\n",
    "            \"[嘻嘻]\",\n",
    "            \"[笑cry]\",\n",
    "            \"[怒]\",\n",
    "            \"[泪]\",\n",
    "            \"[允悲]\",\n",
    "            \"[憧憬]\",\n",
    "            \"[doge]\",\n",
    "            \"[并不简单]\",\n",
    "            \"[思考]\",\n",
    "            \"[费解]\",\n",
    "            \"[吃惊]\",\n",
    "            \"[拜拜]\",\n",
    "            \"[吃瓜]\",\n",
    "            \"[赞]\",\n",
    "            \"[心]\",\n",
    "            \"[伤心]\",\n",
    "            \"[蜡烛]\",\n",
    "            \"[给力]\",\n",
    "            \"[威武]\",\n",
    "            \"[跪了]\",\n",
    "            \"[中国赞]\",\n",
    "            \"[给你小心心]\",\n",
    "            \"[酸]\",\n",
    "        ]\n",
    "\n",
    "        self.id2label = {k: v for k, v in enumerate(label_dic)}  # 用于标签的部分\n",
    "        self.label2id = {v: k for k, v in enumerate(label_dic)}\n",
    "\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.PTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BaselineData(Dataset):\n",
    "    def __init__(self, data, tokenizer, config):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_size = config.pad_size\n",
    "        self.label2id = config.label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = \"[SEP]\".join(\n",
    "            [self.data[idx][\"hashtag\"]] + self.data[idx][\"comments\"]\n",
    "        )\n",
    "\n",
    "        input_ids, attention_mask = self.__convert_to_id__(sentence)\n",
    "\n",
    "        if self.data[idx].get(\"attitudes\"):\n",
    "            label = self.__convert_label__(self.data[idx][\"attitudes\"])\n",
    "            return (\n",
    "                torch.tensor(input_ids),\n",
    "                torch.tensor(attention_mask),\n",
    "                torch.tensor(label),\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.tensor(input_ids),\n",
    "                torch.tensor(attention_mask),\n",
    "            )\n",
    "\n",
    "    def __convert_to_id__(self, sentence):\n",
    "        ids = self.tokenizer.encode_plus(sentence)\n",
    "        input_ids = self.__padding__(ids[\"input_ids\"])\n",
    "        attention_mask = self.__padding__(ids[\"attention_mask\"])\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def __convert_label__(self, label):\n",
    "        onehot_label = [0] * 24\n",
    "        for i in label:\n",
    "            onehot_label[self.label2id[i]] = 1\n",
    "        return onehot_label\n",
    "\n",
    "    def __padding__(self, sentence):\n",
    "        sentence = sentence[: self.pad_size]  # 长就截断\n",
    "        sentence = sentence + [0] * (self.pad_size - len(sentence))  # 短就补充\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0] * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BaselineData(train_data, tokenizer, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
