{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfOKN1k19_JM",
        "outputId": "aa34ed16-2c67-4ab5-8bc0-e1fad98ebc7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_hUbOIkY-fRu"
      },
      "outputs": [],
      "source": [
        "def read_json(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
        "        for line in fp:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ec4yWClZ-fW6"
      },
      "outputs": [],
      "source": [
        "class BaselineData(Dataset):\n",
        "    def __init__(self, data, tokenizer, config):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_size = config.pad_size\n",
        "        self.label2id = config.label2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = \"[SEP]\".join(\n",
        "            [self.data[idx][\"hashtag\"]] + self.data[idx][\"comments\"]\n",
        "        )\n",
        "\n",
        "        input_ids, attention_mask = self.__convert_to_id__(sentence)\n",
        "\n",
        "        if self.data[idx].get(\"attitudes\"):\n",
        "            label = self.__convert_label__(self.data[idx][\"attitudes\"])\n",
        "            return (\n",
        "                torch.tensor(input_ids),\n",
        "                torch.tensor(attention_mask),\n",
        "                torch.tensor(label),\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                torch.tensor(input_ids),\n",
        "                torch.tensor(attention_mask),\n",
        "            )\n",
        "\n",
        "    def __convert_to_id__(self, sentence):\n",
        "        ids = self.tokenizer.encode_plus(sentence)\n",
        "        input_ids = self.__padding__(ids[\"input_ids\"])\n",
        "        attention_mask = self.__padding__(ids[\"attention_mask\"])\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    def __convert_label__(self, label):\n",
        "        onehot_label = [0] * 24\n",
        "        for i in label:\n",
        "            onehot_label[self.label2id[i]] = 1\n",
        "        return onehot_label\n",
        "\n",
        "    def __padding__(self, sentence):\n",
        "        sentence = sentence[: self.pad_size]  # 长就截断\n",
        "        sentence = sentence + [0] * (self.pad_size - len(sentence))  # 短就补充\n",
        "        return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XS5_wE0l-fa6"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.bert = RobertaModel.from_pretrained(config.PTM)\n",
        "        self.bert_config = RobertaConfig.from_pretrained(config.PTM)\n",
        "        self.fc = nn.Linear(self.bert_config.hidden_size, self.bert_config.hidden_size)\n",
        "        self.fc1 = nn.Linear(self.bert_config.hidden_size, config.label_num)\n",
        "        self.act = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bert(input_ids=x[0], attention_mask=x[1]).pooler_output\n",
        "        x = self.sigmoid(self.fc1(self.act(self.fc(x))))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lg9YwUQi-fds"
      },
      "outputs": [],
      "source": [
        "def train(config, dataset, model, optimizer, valid_dataset):\n",
        "    max_scores = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        with tqdm(total=len(dataset)) as pbar:\n",
        "            for idx, data in enumerate(dataset):\n",
        "                x = [data[0].long(), data[1].long()]\n",
        "                y = data[2].float()\n",
        "                y_hat = model(x)\n",
        "                loss = F.binary_cross_entropy(y_hat, y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                pbar.set_postfix({\"loss\": \"{:.4f}\".format(loss)})\n",
        "                pbar.update(1)\n",
        "        scores = valid(config, valid_dataset, model)\n",
        "        if scores >= max_scores:\n",
        "            max_scores = scores\n",
        "            saved_model = model\n",
        "    return saved_model\n",
        "\n",
        "\n",
        "def valid(config, dataset, model):\n",
        "    true = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataset):\n",
        "            x = [data[0].long(), data[1].long()]\n",
        "            y = data[2].float().view(-1, 24).tolist()\n",
        "            y_hat = model(x).view(-1, 24).tolist()\n",
        "            true.extend(y)\n",
        "            pred.extend(y_hat)\n",
        "\n",
        "    pred = [[1 if i >= 0.5 else 0 for i in j] for j in pred]\n",
        "\n",
        "    micro_f1 = f1_score(pred, true, average=\"micro\")\n",
        "    macro_f1 = f1_score(pred, true, average=\"macro\")\n",
        "    print(\"micro_f1: {:.4f}\".format(micro_f1))\n",
        "    print(\"micro_f1: {:.4f}\".format(macro_f1))\n",
        "    return micro_f1 * 0.4 + macro_f1 * 0.6\n",
        "\n",
        "\n",
        "import heapq\n",
        "\n",
        "\n",
        "def generate_test_result(config, dataset, model):\n",
        "    with torch.no_grad():\n",
        "        predict = []\n",
        "        for idx, data in enumerate(dataset):\n",
        "            x = [data[0].long(), data[1].long()]\n",
        "            predict.extend(model(x).view(-1, 24).tolist())\n",
        "    with open(\"submit.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(predict)):\n",
        "            line = []\n",
        "\n",
        "            max_number = heapq.nlargest(3, predict[i])\n",
        "            # max_index = []\n",
        "            for t in max_number:\n",
        "                index = predict[i].index(t)\n",
        "                # max_index.append(index)\n",
        "                line.append(config.id2label[index])\n",
        "\n",
        "            # for j in range(len(predict[i])):\n",
        "            #     if predict[i][j] >= 0.5:\n",
        "            #         line.append(config.id2label[j])\n",
        "\n",
        "            f.write(\" \".join([str(i)] + line))\n",
        "            f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7891892194747925, 0.618352472782135, 0.47227057814598083]\n",
            "[13, 14, 10]\n"
          ]
        }
      ],
      "source": [
        "import heapq\n",
        "\n",
        "m = [\n",
        "    0.016138209030032158,\n",
        "    0.1027437224984169,\n",
        "    0.007721974980086088,\n",
        "    0.015482794493436813,\n",
        "    0.03860097751021385,\n",
        "    0.01681477390229702,\n",
        "    0.0018505491316318512,\n",
        "    0.2727064788341522,\n",
        "    0.10750023275613785,\n",
        "    0.10568385571241379,\n",
        "    0.47227057814598083,\n",
        "    0.059659115970134735,\n",
        "    0.02180691622197628,\n",
        "    0.7891892194747925,\n",
        "    0.618352472782135,\n",
        "    0.23794208467006683,\n",
        "    0.011238433420658112,\n",
        "    0.11568883806467056,\n",
        "    0.07733272016048431,\n",
        "    0.009712048806250095,\n",
        "    0.011674805544316769,\n",
        "    0.12361800670623779,\n",
        "    0.00770289683714509,\n",
        "    0.055381402373313904,\n",
        "]\n",
        "max_number = heapq.nlargest(3, m)\n",
        "max_index = []\n",
        "for t in max_number:\n",
        "    index = m.index(t)\n",
        "    max_index.append(index)\n",
        "print(max_number)\n",
        "print(max_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ItHXTZ7eCPs2"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.pad_size = 500\n",
        "        self.batch_size = 24\n",
        "        self.epochs = 1\n",
        "        self.PTM = \"Yuang/unilm-base-chinese-news-sum\"\n",
        "        self.label_num = 24\n",
        "        self.device = \"cuda:0\"\n",
        "        self.lr = 5e-5\n",
        "\n",
        "        label_dic = [\n",
        "            \"[微笑]\",\n",
        "            \"[嘻嘻]\",\n",
        "            \"[笑cry]\",\n",
        "            \"[怒]\",\n",
        "            \"[泪]\",\n",
        "            \"[允悲]\",\n",
        "            \"[憧憬]\",\n",
        "            \"[doge]\",\n",
        "            \"[并不简单]\",\n",
        "            \"[思考]\",\n",
        "            \"[费解]\",\n",
        "            \"[吃惊]\",\n",
        "            \"[拜拜]\",\n",
        "            \"[吃瓜]\",\n",
        "            \"[赞]\",\n",
        "            \"[心]\",\n",
        "            \"[伤心]\",\n",
        "            \"[蜡烛]\",\n",
        "            \"[给力]\",\n",
        "            \"[威武]\",\n",
        "            \"[跪了]\",\n",
        "            \"[中国赞]\",\n",
        "            \"[给你小心心]\",\n",
        "            \"[酸]\",\n",
        "        ]\n",
        "\n",
        "        self.id2label = {k: v for k, v in enumerate(label_dic)}  # 用于标签的部分\n",
        "        self.label2id = {v: k for k, v in enumerate(label_dic)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NdzMgbaxCTaF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 28.1kB/s]\n",
            "d:\\Program Files\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 356/356 [00:00<00:00, 119kB/s]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'UniLMTokenizer'. \n",
            "The class this function is called from is 'RobertaTokenizer'.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m valid_data \u001b[39m=\u001b[39m read_json(\u001b[39m\"\u001b[39m\u001b[39mvalid.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m test_data \u001b[39m=\u001b[39m read_json(\u001b[39m\"\u001b[39m\u001b[39mtest.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[39m=\u001b[39m RobertaTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(config\u001b[39m.\u001b[39;49mPTM)\n\u001b[0;32m      8\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m      9\u001b[0m     BaselineData(train_data, tokenizer, config), batch_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m valid_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m     12\u001b[0m     BaselineData(valid_data, tokenizer, config), batch_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbatch_size\n\u001b[0;32m     13\u001b[0m )\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1804\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1801\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1802\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1804\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   1805\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1806\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1807\u001b[0m     init_configuration,\n\u001b[0;32m   1808\u001b[0m     \u001b[39m*\u001b[39minit_inputs,\n\u001b[0;32m   1809\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1810\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1811\u001b[0m     local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1812\u001b[0m     _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   1813\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1814\u001b[0m )\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1956\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   1957\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1958\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs)\n\u001b[0;32m   1959\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   1961\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1962\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1963\u001b[0m     )\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta.py:226\u001b[0m, in \u001b[0;36mRobertaTokenizer.__init__\u001b[1;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[0;32m    213\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    214\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[0;32m    215\u001b[0m     bos_token\u001b[39m=\u001b[39mbos_token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    224\u001b[0m )\n\u001b[1;32m--> 226\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(vocab_file, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m vocab_handle:\n\u001b[0;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(vocab_handle)\n\u001b[0;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mitems()}\n",
            "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
          ]
        }
      ],
      "source": [
        "config = Config()\n",
        "train_data = read_json(\"train.json\")\n",
        "valid_data = read_json(\"valid.json\")\n",
        "test_data = read_json(\"test.json\")\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(config.PTM)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    BaselineData(train_data, tokenizer, config), batch_size=config.batch_size\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    BaselineData(valid_data, tokenizer, config), batch_size=config.batch_size\n",
        ")\n",
        "test_dataloader = DataLoader(BaselineData(test_data, tokenizer, config), batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data[8]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data[37]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data[44]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# @ with : and @ without :\n",
        "def clean_at(text):\n",
        "    at_pattern = re.compile(\"@\\S*:\", re.S)\n",
        "    text = re.sub(at_pattern, \"\", text)\n",
        "    at_pattern = re.compile(\"@\\S*\", re.S)\n",
        "    text = re.sub(at_pattern, \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# Clear url in comments\n",
        "def clean_url(text):\n",
        "    sentences = text.split(\" \")\n",
        "    # 处理http://类链接\n",
        "    url_pattern = re.compile(r\"(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b\", re.S)\n",
        "    # 处理无http://类链接\n",
        "    domain_pattern = re.compile(r\"(\\b)*(.*?)\\.(com|cn)\")\n",
        "    if len(sentences) > 0:\n",
        "        result = []\n",
        "        for item in sentences:\n",
        "            text = re.sub(url_pattern, \"\", item)\n",
        "            text = re.sub(domain_pattern, \"\", text)\n",
        "            result.append(text)\n",
        "        return \" \".join(result)\n",
        "    else:\n",
        "        return re.sub(url_pattern, \"\", sentences)\n",
        "\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    for j in range(len(train_data[i][\"comments\"])):\n",
        "        text = train_data[i][\"comments\"][j]\n",
        "        text_clean_at = clean_at(text)\n",
        "        text_clean_url = clean_url(text_clean_at)\n",
        "        train_data[i][\"comments\"][j] = text_clean_url\n",
        "\n",
        "train_data[44]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YldSf9yy-7X3",
        "outputId": "0bbe51f8-c3b9-49e2-8e89-e4769832a01f"
      },
      "outputs": [],
      "source": [
        "model = Model(config)  # .to(config.device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), config.lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWJVjUIf-7aW",
        "outputId": "994d8a68-f072-4c13-cf54-99fa78ad6423"
      },
      "outputs": [],
      "source": [
        "best_model = train(config, train_dataloader, model, optimizer, valid_dataloader)\n",
        "generate_test_result(config, test_dataloader, best_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "label_dic = [\n",
        "    \"[微笑]\",\n",
        "    \"[嘻嘻]\",\n",
        "    \"[笑cry]\",\n",
        "    \"[怒]\",\n",
        "    \"[泪]\",\n",
        "    \"[允悲]\",\n",
        "    \"[憧憬]\",\n",
        "    \"[doge]\",\n",
        "    \"[并不简单]\",\n",
        "    \"[思考]\",\n",
        "    \"[费解]\",\n",
        "    \"[吃惊]\",\n",
        "    \"[拜拜]\",\n",
        "    \"[吃瓜]\",\n",
        "    \"[赞]\",\n",
        "    \"[心]\",\n",
        "    \"[伤心]\",\n",
        "    \"[蜡烛]\",\n",
        "    \"[给力]\",\n",
        "    \"[威武]\",\n",
        "    \"[跪了]\",\n",
        "    \"[中国赞]\",\n",
        "    \"[给你小心心]\",\n",
        "    \"[酸]\",\n",
        "]\n",
        "id2label = {k: v for k, v in enumerate(label_dic)}  # 用于标签的部分\n",
        "label2id = {v: k for k, v in enumerate(label_dic)}\n",
        "\n",
        "\n",
        "def convert_label(fn_result):\n",
        "    convert_label = []\n",
        "\n",
        "    for line in open(fn_result, \"r\", encoding=\"utf-8\"):\n",
        "        labellist = line.strip().split(\" \")[1:]\n",
        "\n",
        "        onehot_label = [0] * 24\n",
        "        for label in labellist:\n",
        "            onehot_label[label2id[label]] = 1\n",
        "        convert_label.append(onehot_label)\n",
        "    return convert_label\n",
        "\n",
        "\n",
        "def score_calculation(pred, true):\n",
        "    macro_f1 = f1_score(pred, true, average=\"macro\")\n",
        "    return macro_f1\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Generate classification_report from given pred and gold tsv files.\n",
        "    \"\"\"\n",
        "\n",
        "    # Usage:\n",
        "    # python evaluate PREDICTION_FILE GOLD_ANSWER_FILE\n",
        "    # Example:\n",
        "    # python evaluate pred.tsv gold.tsv\n",
        "    print(len(sys.argv))\n",
        "    print(\"The list of command line arguments:\\n\", sys.argv)\n",
        "\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Please indicate the prediction and gold tsvs.\")\n",
        "        quit()\n",
        "    else:\n",
        "        pred_fn = sys.argv[1]\n",
        "        gold_fn = sys.argv[2]\n",
        "\n",
        "    print(\"Loading the datasets ...\")\n",
        "    pred_lbl = convert_label(pred_fn)\n",
        "    gold_lbl = convert_label(gold_fn)\n",
        "\n",
        "    print(\"Evaluating ...\")\n",
        "    try:\n",
        "        macro_f1 = score_calculation(pred_lbl, gold_lbl)\n",
        "\n",
        "        print(\"macro_f1: {:.4f}\".format(macro_f1))\n",
        "    except Exception as ex:\n",
        "        print(\"error:\", ex)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
