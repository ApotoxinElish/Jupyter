{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfOKN1k19_JM",
        "outputId": "aa34ed16-2c67-4ab5-8bc0-e1fad98ebc7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertConfig, BertModel, BertTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_hUbOIkY-fRu"
      },
      "outputs": [],
      "source": [
        "def read_json(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as fp:\n",
        "        for line in fp:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ec4yWClZ-fW6"
      },
      "outputs": [],
      "source": [
        "class BaselineData(Dataset):\n",
        "    def __init__(self, data, tokenizer, config):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_size = config.pad_size\n",
        "        self.label2id = config.label2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = \"[SEP]\".join(\n",
        "            [self.data[idx][\"hashtag\"]] + self.data[idx][\"comments\"]\n",
        "        )\n",
        "\n",
        "        input_ids, attention_mask = self.__convert_to_id__(sentence)\n",
        "\n",
        "        if self.data[idx].get(\"attitudes\"):\n",
        "            label = self.__convert_label__(self.data[idx][\"attitudes\"])\n",
        "            return (\n",
        "                torch.tensor(input_ids),\n",
        "                torch.tensor(attention_mask),\n",
        "                torch.tensor(label),\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                torch.tensor(input_ids),\n",
        "                torch.tensor(attention_mask),\n",
        "            )\n",
        "\n",
        "    def __convert_to_id__(self, sentence):\n",
        "        ids = self.tokenizer.encode_plus(sentence)\n",
        "        input_ids = self.__padding__(ids[\"input_ids\"])\n",
        "        attention_mask = self.__padding__(ids[\"attention_mask\"])\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    def __convert_label__(self, label):\n",
        "        onehot_label = [0] * 24\n",
        "        for i in label:\n",
        "            onehot_label[self.label2id[i]] = 1\n",
        "        return onehot_label\n",
        "\n",
        "    def __padding__(self, sentence):\n",
        "        sentence = sentence[: self.pad_size]  # é•¿å°±æˆªæ–­\n",
        "        sentence = sentence + [0] * (self.pad_size - len(sentence))  # çŸ­å°±è¡¥å……\n",
        "        return sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XS5_wE0l-fa6"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.bert = BertModel.from_pretrained(config.PTM)\n",
        "        self.bert_config = BertConfig.from_pretrained(config.PTM)\n",
        "        self.fc = nn.Linear(self.bert_config.hidden_size, self.bert_config.hidden_size)\n",
        "        self.fc1 = nn.Linear(self.bert_config.hidden_size, config.label_num)\n",
        "        self.act = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bert(input_ids=x[0], attention_mask=x[1]).pooler_output\n",
        "        x = self.sigmoid(self.fc1(self.act(self.fc(x))))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lg9YwUQi-fds"
      },
      "outputs": [],
      "source": [
        "def train(config, dataset, model, optimizer, valid_dataset):\n",
        "    max_scores = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        with tqdm(total=len(dataset)) as pbar:\n",
        "            for idx, data in enumerate(dataset):\n",
        "                x = [data[0].long(), data[1].long()]\n",
        "                y = data[2].float()\n",
        "                y_hat = model(x)\n",
        "                loss = F.binary_cross_entropy(y_hat, y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                pbar.set_postfix({\"loss\": \"{:.4f}\".format(loss)})\n",
        "                pbar.update(1)\n",
        "        scores = valid(config, valid_dataset, model)\n",
        "        if scores >= max_scores:\n",
        "            max_scores = scores\n",
        "            saved_model = model\n",
        "    return saved_model\n",
        "\n",
        "\n",
        "def valid(config, dataset, model):\n",
        "    true = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataset):\n",
        "            x = [data[0].long(), data[1].long()]\n",
        "            y = data[2].float().view(-1, 24).tolist()\n",
        "            y_hat = model(x).view(-1, 24).tolist()\n",
        "            true.extend(y)\n",
        "            pred.extend(y_hat)\n",
        "\n",
        "    pred = [[1 if i >= 0.5 else 0 for i in j] for j in pred]\n",
        "\n",
        "    micro_f1 = f1_score(pred, true, average=\"micro\")\n",
        "    macro_f1 = f1_score(pred, true, average=\"macro\")\n",
        "    print(\"micro_f1: {:.4f}\".format(micro_f1))\n",
        "    print(\"micro_f1: {:.4f}\".format(macro_f1))\n",
        "    return micro_f1 * 0.4 + macro_f1 * 0.6\n",
        "\n",
        "\n",
        "import heapq\n",
        "\n",
        "\n",
        "def generate_test_result(config, dataset, model):\n",
        "    with torch.no_grad():\n",
        "        predict = []\n",
        "        for idx, data in enumerate(dataset):\n",
        "            x = [data[0].long(), data[1].long()]\n",
        "            predict.extend(model(x).view(-1, 24).tolist())\n",
        "    with open(\"submit.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(predict)):\n",
        "            line = []\n",
        "\n",
        "            max_number = heapq.nlargest(3, predict[i])\n",
        "            # max_index = []\n",
        "            for t in max_number:\n",
        "                index = predict[i].index(t)\n",
        "                # max_index.append(index)\n",
        "                line.append(config.id2label[index])\n",
        "\n",
        "            # for j in range(len(predict[i])):\n",
        "            #     if predict[i][j] >= 0.5:\n",
        "            #         line.append(config.id2label[j])\n",
        "\n",
        "            f.write(\" \".join([str(i)] + line))\n",
        "            f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7891892194747925, 0.618352472782135, 0.47227057814598083]\n",
            "[13, 14, 10]\n"
          ]
        }
      ],
      "source": [
        "import heapq\n",
        "\n",
        "m = [\n",
        "    0.016138209030032158,\n",
        "    0.1027437224984169,\n",
        "    0.007721974980086088,\n",
        "    0.015482794493436813,\n",
        "    0.03860097751021385,\n",
        "    0.01681477390229702,\n",
        "    0.0018505491316318512,\n",
        "    0.2727064788341522,\n",
        "    0.10750023275613785,\n",
        "    0.10568385571241379,\n",
        "    0.47227057814598083,\n",
        "    0.059659115970134735,\n",
        "    0.02180691622197628,\n",
        "    0.7891892194747925,\n",
        "    0.618352472782135,\n",
        "    0.23794208467006683,\n",
        "    0.011238433420658112,\n",
        "    0.11568883806467056,\n",
        "    0.07733272016048431,\n",
        "    0.009712048806250095,\n",
        "    0.011674805544316769,\n",
        "    0.12361800670623779,\n",
        "    0.00770289683714509,\n",
        "    0.055381402373313904,\n",
        "]\n",
        "max_number = heapq.nlargest(3, m)\n",
        "max_index = []\n",
        "for t in max_number:\n",
        "    index = m.index(t)\n",
        "    max_index.append(index)\n",
        "print(max_number)\n",
        "print(max_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ItHXTZ7eCPs2"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.pad_size = 500\n",
        "        self.batch_size = 24\n",
        "        self.epochs = 1\n",
        "        self.PTM = \"Yuang/unilm-base-chinese-news-sum\"\n",
        "        self.label_num = 24\n",
        "        self.device = \"cuda:0\"\n",
        "        self.lr = 5e-5\n",
        "\n",
        "        label_dic = [\n",
        "            \"[å¾®ç¬‘]\",\n",
        "            \"[å˜»å˜»]\",\n",
        "            \"[ç¬‘cry]\",\n",
        "            \"[æ€’]\",\n",
        "            \"[æ³ª]\",\n",
        "            \"[å…æ‚²]\",\n",
        "            \"[æ†§æ†¬]\",\n",
        "            \"[doge]\",\n",
        "            \"[å¹¶ä¸ç®€å•]\",\n",
        "            \"[æ€è€ƒ]\",\n",
        "            \"[è´¹è§£]\",\n",
        "            \"[åƒæƒŠ]\",\n",
        "            \"[æ‹œæ‹œ]\",\n",
        "            \"[åƒç“œ]\",\n",
        "            \"[èµ]\",\n",
        "            \"[å¿ƒ]\",\n",
        "            \"[ä¼¤å¿ƒ]\",\n",
        "            \"[èœ¡çƒ›]\",\n",
        "            \"[ç»™åŠ›]\",\n",
        "            \"[å¨æ­¦]\",\n",
        "            \"[è·ªäº†]\",\n",
        "            \"[ä¸­å›½èµ]\",\n",
        "            \"[ç»™ä½ å°å¿ƒå¿ƒ]\",\n",
        "            \"[é…¸]\",\n",
        "        ]\n",
        "\n",
        "        self.id2label = {k: v for k, v in enumerate(label_dic)}  # ç”¨äºæ ‡ç­¾çš„éƒ¨åˆ†\n",
        "        self.label2id = {v: k for k, v in enumerate(label_dic)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NdzMgbaxCTaF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110k/110k [00:00<00:00, 10.3MB/s]\n",
            "d:\\Program Files\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'UniLMTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "config = Config()\n",
        "train_data = read_json(\"train.json\")\n",
        "valid_data = read_json(\"valid.json\")\n",
        "test_data = read_json(\"test.json\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(config.PTM)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    BaselineData(train_data, tokenizer, config), batch_size=config.batch_size\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    BaselineData(valid_data, tokenizer, config), batch_size=config.batch_size\n",
        ")\n",
        "test_dataloader = DataLoader(BaselineData(test_data, tokenizer, config), batch_size=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': 'ä»»å˜‰ä¼¦è°­æ¾éŸµäº’å«æœ¬å',\n",
              " 'attitudes': ['[ç¬‘cry]', '[å…æ‚²]', '[å¿ƒ]'],\n",
              " 'comments': ['æ™¶æ™¶å•Šæ™¶æ™¶å•Š',\n",
              "  'å“ˆå“ˆå“ˆå“ˆå“ˆå“ˆä¸¤ä¸ªäººå¥½å¯çˆ±',\n",
              "  'çªç„¶è§‰å¾—è‡ªå·±çœ‹é”¦è¡£ä¹‹ä¸‹ä¹‹å‰å°±æ˜¯ä¸ªçå­',\n",
              "  'å–Šäº†å››ä¸ªæ™¶æ™¶å•Šï¼Œå› ä¸ºé”¦è¡£ä¹‹ä¸‹ï¼Œå› ä¸ºé™†å¤§äººä»Šå¤ï¼Œå˜‰ä¼¦æ¾éŸµæˆäº†å®¶äººä¸€æ ·çš„äº²åˆ‡å…³ç³»ï¼Œç¥ç¦å‹è°Šä¹‹æ ‘é•¿é’ã€‚@ä»»å˜‰ä¼¦Allen @è°­æ¾éŸµseven @é”¦è¡£ä¹‹ä¸‹ç”µè§†å‰§',\n",
              "  'å›½è¶…â€œæ™¶æ™¶å•Šï¼Œæ™¶æ™¶å•Šï¼Œæ™¶æ™¶å•Šâ€ï¼Œæ™¶æ™¶â€œå“â€ï¼Œå›½è¶…â€œä½ å¯ä»¥æœä¸€ä¸‹ä»»å˜‰ä¼¦â€ï¼Œæ™¶æ™¶â€œæˆ‘å—–å•¦â€ã€‚']}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[8]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': 'æ­¦æ±‰ä¸€é¤å…ç–«æƒ…æœŸé€8000å¤šä»½çˆ±å¿ƒé¤',\n",
              " 'attitudes': ['[å¿ƒ]', '[æ³ª]', '[èµ]'],\n",
              " 'comments': ['é¤å…åå­—ï¼Œä¸‰ä¸²è‚†å­£  ç‹å®¶å¢© é€æˆ‘ä¸Šå»',\n",
              "  'æ´»è¯¥ä½ åº—ç«ğŸ”¥ä½ çŸ¥é“å§',\n",
              "  'å¾ˆæ£’çš„æ¯äº²',\n",
              "  'æ€»æœ‰äººç«­åŠ›æ¸©æš–è¿™ä¸ªä¸–ç•Œï¼è‡´æ•¬ï¼å¯æ•¬çš„æ¯äº²',\n",
              "  'å¯æ•¬çš„å¦ˆå¦ˆï¼Œæ‚¨çš„å­©å­ä¸€å®šä»¥ä½ ä¸ºæ¦œæ ·çš„ï¼Œæ¯äº²èŠ‚å¿«ä¹â¤ï¸']}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[37]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': 'è¿‘è§†çœ¼çš„æ—¥å¸¸',\n",
              " 'attitudes': ['[å…æ‚²]', '[æ³ª]', '[doge]'],\n",
              " 'comments': ['è„‘æ´ç¿»å”± ã€wherever you areâ†’http://t.cn/AiO9oJhZã€‘ ã€è‚¥å®…çš„æƒ°æ€§â†’http://t.cn/AiOSBn6mã€‘ã€æ˜Ÿå·´å…‹çš„çº¦å®šâ†’http://t.cn/AiWqi9aBã€‘ ã€åƒè´§è‚¥å®…ä»¬çš„æ—¥å¸¸â†’http://t.cn/AiWqi9amã€‘ã€ç†Šå­©çº¸â†’ http://t.cn/AiF2FclTã€‘ã€ç†å‘åº—â†’ http://t.cn/AiF2FclNã€‘ã€åšé¥­éš¾åƒâ†’ http://t.cn/A6hSy9ZYã€‘',\n",
              "  'ä½ èªªä»€éº¼ç­‰ç­‰æˆ‘æ²’æˆ´çœ¼é¡è½ä¸æ¸…',\n",
              "  'ä»¥å‰åšè§†åŠ›æ£€æŸ¥ï¼Œé—®è¿‡åŒ»ç”Ÿï¼ŒæŒ‡è§†åŠ›è¡¨çš„æ£å­åœ¨å“ªï¼ˆçœ¯çœ¼',\n",
              "  'æ›¾ç»æ²¡æˆ´çœ¼é•œï¼Œä»¥ä¸ºæ˜¯å¶å­ï¼Œç»“æœå¾’æ‰‹æŠ“äº†åªå£è™',\n",
              "  '  â€œå¤œæ™šçš„ç¯å…‰å¥½ç¾å•Š çœ‹èµ·æ¥åƒå½©ç¯ç§€â€å¤ªçœŸå®äº†å§ï¼ï¼ï¼æ•£å…‰çœ¼è¡¨ç¤ºæ‘˜ä¸‹çœ¼é•œ å¯ä»¥è¢«è·¯ç¯æ™ƒæ™•']}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[44]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hashtag': 'è¿‘è§†çœ¼çš„æ—¥å¸¸',\n",
              " 'attitudes': ['[å…æ‚²]', '[æ³ª]', '[doge]'],\n",
              " 'comments': ['è„‘æ´ç¿»å”± ã€wherever you areâ†’ã€‘ ã€è‚¥å®…çš„æƒ°æ€§â†’ã€‘ã€æ˜Ÿå·´å…‹çš„çº¦å®šâ†’ã€‘ ã€åƒè´§è‚¥å®…ä»¬çš„æ—¥å¸¸â†’ã€‘ã€ç†Šå­©çº¸â†’ ã€‘ã€ç†å‘åº—â†’ ã€‘ã€åšé¥­éš¾åƒâ†’ ã€‘',\n",
              "  'ä½ èªªä»€éº¼ç­‰ç­‰æˆ‘æ²’æˆ´çœ¼é¡è½ä¸æ¸…',\n",
              "  'ä»¥å‰åšè§†åŠ›æ£€æŸ¥ï¼Œé—®è¿‡åŒ»ç”Ÿï¼ŒæŒ‡è§†åŠ›è¡¨çš„æ£å­åœ¨å“ªï¼ˆçœ¯çœ¼',\n",
              "  'æ›¾ç»æ²¡æˆ´çœ¼é•œï¼Œä»¥ä¸ºæ˜¯å¶å­ï¼Œç»“æœå¾’æ‰‹æŠ“äº†åªå£è™',\n",
              "  'â€œå¤œæ™šçš„ç¯å…‰å¥½ç¾å•Š çœ‹èµ·æ¥åƒå½©ç¯ç§€â€å¤ªçœŸå®äº†å§ï¼ï¼ï¼æ•£å…‰çœ¼è¡¨ç¤ºæ‘˜ä¸‹çœ¼é•œ å¯ä»¥è¢«è·¯ç¯æ™ƒæ™•']}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# @ with : and @ without :\n",
        "def clean_at(text):\n",
        "    at_pattern = re.compile(\"@\\S*:\", re.S)\n",
        "    text = re.sub(at_pattern, \"\", text)\n",
        "    at_pattern = re.compile(\"@\\S*\", re.S)\n",
        "    text = re.sub(at_pattern, \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "# Clear url in comments\n",
        "def clean_url(text):\n",
        "    sentences = text.split(\" \")\n",
        "    # å¤„ç†http://ç±»é“¾æ¥\n",
        "    url_pattern = re.compile(r\"(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b\", re.S)\n",
        "    # å¤„ç†æ— http://ç±»é“¾æ¥\n",
        "    domain_pattern = re.compile(r\"(\\b)*(.*?)\\.(com|cn)\")\n",
        "    if len(sentences) > 0:\n",
        "        result = []\n",
        "        for item in sentences:\n",
        "            text = re.sub(url_pattern, \"\", item)\n",
        "            text = re.sub(domain_pattern, \"\", text)\n",
        "            result.append(text)\n",
        "        return \" \".join(result)\n",
        "    else:\n",
        "        return re.sub(url_pattern, \"\", sentences)\n",
        "\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    for j in range(len(train_data[i][\"comments\"])):\n",
        "        text = train_data[i][\"comments\"][j]\n",
        "        text_clean_at = clean_at(text)\n",
        "        text_clean_url = clean_url(text_clean_at)\n",
        "        train_data[i][\"comments\"][j] = text_clean_url\n",
        "\n",
        "train_data[44]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YldSf9yy-7X3",
        "outputId": "0bbe51f8-c3b9-49e2-8e89-e4769832a01f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 955/955 [00:00<00:00, 957kB/s]\n",
            "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 412M/412M [00:20<00:00, 19.9MB/s] \n",
            "Some weights of the model checkpoint at Yuang/unilm-base-chinese-news-sum were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = Model(config)  # .to(config.device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), config.lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWJVjUIf-7aW",
        "outputId": "994d8a68-f072-4c13-cf54-99fa78ad6423"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|â–ˆâ–        | 37/300 [1:21:33<9:23:52, 128.64s/it, loss=0.3423] "
          ]
        }
      ],
      "source": [
        "best_model = train(config, train_dataloader, model, optimizer, valid_dataloader)\n",
        "generate_test_result(config, test_dataloader, best_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "label_dic = [\n",
        "    \"[å¾®ç¬‘]\",\n",
        "    \"[å˜»å˜»]\",\n",
        "    \"[ç¬‘cry]\",\n",
        "    \"[æ€’]\",\n",
        "    \"[æ³ª]\",\n",
        "    \"[å…æ‚²]\",\n",
        "    \"[æ†§æ†¬]\",\n",
        "    \"[doge]\",\n",
        "    \"[å¹¶ä¸ç®€å•]\",\n",
        "    \"[æ€è€ƒ]\",\n",
        "    \"[è´¹è§£]\",\n",
        "    \"[åƒæƒŠ]\",\n",
        "    \"[æ‹œæ‹œ]\",\n",
        "    \"[åƒç“œ]\",\n",
        "    \"[èµ]\",\n",
        "    \"[å¿ƒ]\",\n",
        "    \"[ä¼¤å¿ƒ]\",\n",
        "    \"[èœ¡çƒ›]\",\n",
        "    \"[ç»™åŠ›]\",\n",
        "    \"[å¨æ­¦]\",\n",
        "    \"[è·ªäº†]\",\n",
        "    \"[ä¸­å›½èµ]\",\n",
        "    \"[ç»™ä½ å°å¿ƒå¿ƒ]\",\n",
        "    \"[é…¸]\",\n",
        "]\n",
        "id2label = {k: v for k, v in enumerate(label_dic)}  # ç”¨äºæ ‡ç­¾çš„éƒ¨åˆ†\n",
        "label2id = {v: k for k, v in enumerate(label_dic)}\n",
        "\n",
        "\n",
        "def convert_label(fn_result):\n",
        "    convert_label = []\n",
        "\n",
        "    for line in open(fn_result, \"r\", encoding=\"utf-8\"):\n",
        "        labellist = line.strip().split(\" \")[1:]\n",
        "\n",
        "        onehot_label = [0] * 24\n",
        "        for label in labellist:\n",
        "            onehot_label[label2id[label]] = 1\n",
        "        convert_label.append(onehot_label)\n",
        "    return convert_label\n",
        "\n",
        "\n",
        "def score_calculation(pred, true):\n",
        "    macro_f1 = f1_score(pred, true, average=\"macro\")\n",
        "    return macro_f1\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Generate classification_report from given pred and gold tsv files.\n",
        "    \"\"\"\n",
        "\n",
        "    # Usage:\n",
        "    # python evaluate PREDICTION_FILE GOLD_ANSWER_FILE\n",
        "    # Example:\n",
        "    # python evaluate pred.tsv gold.tsv\n",
        "    print(len(sys.argv))\n",
        "    print(\"The list of command line arguments:\\n\", sys.argv)\n",
        "\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Please indicate the prediction and gold tsvs.\")\n",
        "        quit()\n",
        "    else:\n",
        "        pred_fn = sys.argv[1]\n",
        "        gold_fn = sys.argv[2]\n",
        "\n",
        "    print(\"Loading the datasets ...\")\n",
        "    pred_lbl = convert_label(pred_fn)\n",
        "    gold_lbl = convert_label(gold_fn)\n",
        "\n",
        "    print(\"Evaluating ...\")\n",
        "    try:\n",
        "        macro_f1 = score_calculation(pred_lbl, gold_lbl)\n",
        "\n",
        "        print(\"macro_f1: {:.4f}\".format(macro_f1))\n",
        "    except Exception as ex:\n",
        "        print(\"error:\", ex)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
