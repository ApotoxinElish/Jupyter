{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpJ88jPIikDx"
      },
      "source": [
        "# Image classification with Vision Transformer\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2021/01/18<br>\n",
        "**Last modified:** 2021/01/18<br>\n",
        "**Description:** Implementing the Vision Transformer (ViT) model for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTdZx8-RikDz"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
        "model by Alexey Dosovitskiy et al. for image classification,\n",
        "and demonstrates it on the CIFAR-100 dataset.\n",
        "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
        "image patches, without using convolution layers.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher, as well as\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
        "which can be installed using the following command:\n",
        "\n",
        "```python\n",
        "pip install -U tensorflow-addons\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_1cvfyCikDz"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lsgwiQL8ikDz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Program Files\\Python310\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEBj3LeFikD0"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tn_D3nq1ikD0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFBQ9QVXikD0"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wUZ7jCSXikD0"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icpz-LKBikD1"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J7Yv-QMuikD1"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDnVx69DikD1"
      },
      "source": [
        "## Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S0dL6cPzikD1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NToYf0zikD1"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WLI8UDqEikD1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur1dMSPxikD2"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lv1IwAYsikD2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 144\n",
            "Elements per patch: 108\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 144 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a9msk4ikD2"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
        "vector of size `projection_dim`. In addition, it adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "83Knmf3xikD2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMlw5suyikD2"
      },
      "source": [
        "## Build the ViT model\n",
        "\n",
        "The ViT model consists of multiple Transformer blocks,\n",
        "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
        "applied to the sequence of patches. The Transformer blocks produce a\n",
        "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
        "classifier head with softmax to produce the final class probabilities output.\n",
        "\n",
        "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
        "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
        "as the image representation, all the outputs of the final Transformer block are\n",
        "reshaped with `layers.Flatten()` and used as the image\n",
        "representation input to the classifier head.\n",
        "Note that the `layers.GlobalAveragePooling1D` layer\n",
        "could also be used instead to aggregate the outputs of the Transformer block,\n",
        "especially when the number of patches and the projection dimensions are large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YMJ8kTobikD2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBA2mk8ikD3"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jvXaG91qikD3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 21/176 [==>...........................] - ETA: 12:15 - loss: 5.3205 - accuracy: 0.0190 - top-5-accuracy: 0.0804"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\GitHub\\Jupyter\\CMSC5707\\image_classification_with_vision_transformer.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m history\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m vit_classifier \u001b[39m=\u001b[39m create_vit_classifier()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m history \u001b[39m=\u001b[39m run_experiment(vit_classifier)\n",
            "\u001b[1;32md:\\GitHub\\Jupyter\\CMSC5707\\image_classification_with_vision_transformer.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m checkpoint_filepath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/tmp/checkpoint\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     checkpoint_filepath,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x\u001b[39m=\u001b[39;49mx_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     y\u001b[39m=\u001b[39;49my_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(checkpoint_filepath)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/Jupyter/CMSC5707/image_classification_with_vision_transformer.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m _, accuracy, top_5_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
            "File \u001b[1;32md:\\Program Files\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70m5DutOikD3"
      },
      "source": [
        "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
        "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
        "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
        "\n",
        "Note that the state of the art results reported in the\n",
        "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
        "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
        "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
        "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions.\n",
        "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices,\n",
        "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
        "In practice, it's recommended to fine-tune a ViT model\n",
        "that was pre-trained using a large, high-resolution dataset."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_classification_with_vision_transformer",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
